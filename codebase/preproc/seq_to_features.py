import os
import sys
from pathlib import Path

import joblib
import pandas as pd
import umap
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt

path_root = Path(__file__).parents[1]  # upto 'codebase' folder
sys.path.insert(0, str(path_root))
# print(sys.path)

from utils import preproc_util


# parse the content of allSeqs.fasta and create a dataframe containing 'prot_id' and 'seq' columns
def parse_human_to_fasta(root_path='./'):
    f = open(os.path.join(root_path, 'dataset/orig_data/Human2021', 'allSeqs.fasta'))
    prot_lst, seq_lst = [], []
    idx = 0
    for line in f:
        if idx == 0:
            prot_lst.append(line.strip().strip('>'))
        elif idx == 1:
            seq_lst.append(line.strip())
        idx += 1
        idx = idx % 2
    f.close()

    # create dataframe
    human_seq_df = pd.DataFrame(data = {'prot_id': prot_lst, 'seq': seq_lst})
    # save human_seq_df
    human_seq_df.to_csv(os.path.join(root_path, 'dataset/preproc_data', 'human_seq.csv'), index=False)
    # return human_seq_df
    return human_seq_df

# add features generated by the protTrans model (tl model) to the already saved human_sequence list 
def add_protTrans_feat_to_human_seq(root_path='./', protTrans_model_path='./', protTrans_model_name = 'prot_t5_xl_uniref50'):
    # fetch the already saved human_sequence df
    print('\n ########## fetch the already saved human_sequence df ########## ')
    human_seq_df = pd.read_csv(os.path.join(root_path,'dataset/preproc_data', 'human_seq.csv'))
    # extract features using the protTrans model (tl model) for the human_sequence list
    print('\n ########## extract features using the protTrans model (tl model) for the human_sequence list ########## ')
    features_lst, features_2d_lst = preproc_util.extract_feat_from_protTrans(human_seq_df['seq'].tolist(), protTrans_model_path, protTrans_model_name)
    # use the extracted features alongwith human_seq_df to create a dictionary to be saved as pkl file
    print('\n ########## use the extracted features alongwith human_seq_df to create a dictionary to be saved as pkl file ########## ')
    human_seq_feat_dict = {}
    for index, row in human_seq_df.iterrows():
        human_seq_feat_dict[row['prot_id']] = {'seq': row['seq'], 'seq_feat': features_lst[index], 'seq_2d_feat': features_2d_lst[index]}
    # save human_seq_feat_dict to a .pkl file
    print("\n Saving human_seq_feat_dict to a .pkl file...")
    filename = os.path.join(root_path, 'dataset/preproc_data', 'human_seq_feat_dict_' + protTrans_model_name + '.pkl')
    joblib.dump(value=human_seq_feat_dict, filename=filename, compress=3)
    print("\n The human_seq_feat_dict is saved as: " + filename)
    print("\n######## cleaning all the intermediate stuffs - START ########")
    # remove all the intermediate files in the 'temp_result' and 'temp_per_prot_emb_result' directories which
    # were used in extract_feat_from_preloaded_protTrans() method
    temp_result_dir = os.path.join('temp_result') 
    for temp_file in os.listdir(temp_result_dir):
        os.remove(os.path.join(temp_result_dir, temp_file))
    temp_per_prot_emb_result_dir = os.path.join('temp_per_prot_emb_result') 
    for temp_file in os.listdir(temp_per_prot_emb_result_dir):
        os.remove(os.path.join(temp_per_prot_emb_result_dir, temp_file))
    print("######## cleaning all the intermediate stuffs - DONE ########")


# regularize the 2d features per protein, so that in a protein, per residue feature is dimensionally reduced from
# 1024 to 18 and also each protein length is kept fixed at 2000 either by subsetting or padding similar to PIPR architecture.
# So, after the regularization, each protein can be homogeneously represented by a 2d array of shape 2000 x 18.
def regularize_prot_2d_feat(root_path='./', protTrans_model_name = 'prot_t5_xl_uniref50'):
    print('\n ########## Regularizing the 2d features per protein')
    reduc_dim = 32
    fix_length = 2000

    # load the human_seq_feat_dict
    print('loading the human_seq_feat_dict...')
    human_seq_feat_dict_path = os.path.join(root_path, 'dataset/preproc_data', 'human_seq_feat_dict_' + protTrans_model_name + '.pkl')
    human_seq_feat_dict = joblib.load(human_seq_feat_dict_path)

    # iterate over human_seq_feat_dict and perform regularization per protein i.e.per key
    print('iterating over human_seq_feat_dict and perform the regularization for each protein i.e. for each key ...')
    human_seq_feat_2d_reg_dict = {}
    for prot_id in human_seq_feat_dict:
        print('\n ### prot_id = ' + str(prot_id))
        prot_2d_feat_arr =  human_seq_feat_dict[prot_id]['seq_2d_feat']
        actual_prot_len = len(prot_2d_feat_arr)
        print('actual_prot_len = ' + str(actual_prot_len))

        # # normalize feature(column) wise
        # print('Normalizing feature(column) wise')
        # scaler = preprocessing.MinMaxScaler()
        # prot_2d_feat_arr_scaled = scaler.fit_transform(prot_2d_feat_arr)

        # use UMAP to reduce the per residue feature from 1024 to reduc_dim
        # perform the dimensionality reduction using UMAP
        print('Using UMAP to reduce the per residue feature from 1024 to ' + str(reduc_dim))
        # initiate the umap
        umap_init = 'spectral' if(reduc_dim <= (actual_prot_len-2)) else 'random'
        dim_reducer = umap.UMAP(n_components=reduc_dim, low_memory=False, n_epochs=400, densmap=False
                                , init=umap_init, n_neighbors=30, random_state=456, verbose=True)
        # apply fit and transform on the prot_2d_feat_arr
        prot_2d_feat_arr_reduced = dim_reducer.fit_transform(prot_2d_feat_arr)
        # now fix the the protein length at 2000
        print('Now fixing the the protein length at 2000 ...')
        if actual_prot_len > fix_length:
            prot_2d_feat_arr_reduced =  prot_2d_feat_arr_reduced[:fix_length]
        elif actual_prot_len < fix_length:
            prot_2d_feat_arr_reduced =  np.concatenate((prot_2d_feat_arr_reduced, np.zeros((fix_length - actual_prot_len, reduc_dim))))
        # add the regularized 2d features array of dimension 2000 X 18 to human_seq_feat_2d_reg_dict
        human_seq_feat_2d_reg_dict[prot_id] = prot_2d_feat_arr_reduced

    # save the human_seq_feat_2d_reg_dict as pkl file
    print("\n Saving human_seq_feat_2d_reg_dict to a .pkl file...")
    filename = os.path.join(root_path, 'dataset/preproc_data', 'human_seq_feat_2d_reg_dict.pkl')
    joblib.dump(value=human_seq_feat_2d_reg_dict, filename=filename, compress=3)
    print("\n The human_seq_feat_2d_reg_dict is saved as: " + filename)


# THIS IS AN AD-HOC VERSION OF THE ORIGINAL regularize_prot_2d_feat() METHOD AS IN CASE OF THE DATASET CONTAINING A LARGE NUMBER OF PROTEINS,
# INTERMEDIATE RESULSTS NEED TO BE SAVED AND LATER NEED TO BE MERGED MANUALLY TO GET THE FINAL RESULT (because the program can be abruptly terminated 
# many times and needs to be resumed from the last saved result).
def regularize_prot_2d_feat_AD_HOC(root_path='./', protTrans_model_name = 'prot_t5_xl_uniref50'):
    print('\n ########## Regularizing the 2d features per protein')
    reduc_dim = 100
    fix_length = 2000

    # load the human_seq_feat_dict
    print('loading the human_seq_feat_dict...')
    human_seq_feat_dict_path = os.path.join(root_path, 'dataset/preproc_data', 'human_seq_feat_dict_' + protTrans_model_name + '.pkl')
    human_seq_feat_dict = joblib.load(human_seq_feat_dict_path)

    # trimming human_seq_feat_dict so that it occupies less memory (RAM)
    for prot_id in list(human_seq_feat_dict.keys()):
        human_seq_feat_dict[prot_id]['seq'] = human_seq_feat_dict[prot_id]['seq_feat'] = None

    # iterate over human_seq_feat_dict and perform regularization per protein i.e.per key
    print('iterating over human_seq_feat_dict and perform the regularization for each protein i.e. for each key ...')
    # get the prot_id_lst
    prot_id_lst = list(human_seq_feat_dict.keys())
    total_no_of_proteins = len(prot_id_lst)
    loop_start_index = 3001 # 0  # ##### this can be hardcoded in case of the for loop restart 
    loop_end_index = 6001  # total_no_of_proteins  # ##### this can be hardcoded in case of the for loop restart 
    human_seq_feat_2d_reg_dict = {}

    # for itr in range(loop_start_index, total_no_of_proteins):
    for itr in range(loop_start_index, loop_end_index):
        prot_id = prot_id_lst[itr]
        print('\n### prot_id = ' + str(prot_id))
        prot_2d_feat_arr =  human_seq_feat_dict[prot_id]['seq_2d_feat']
        actual_prot_len = len(prot_2d_feat_arr)
        print('actual_prot_len = ' + str(actual_prot_len))

        # # normalize feature(column) wise
        # print('Normalizing feature(column) wise')
        # scaler = preprocessing.MinMaxScaler()
        # prot_2d_feat_arr_scaled = scaler.fit_transform(prot_2d_feat_arr)

        # use UMAP to reduce the per residue feature from 1024 to reduc_dim
        # perform the dimensionality reduction using UMAP
        print('Using UMAP to reduce the per residue feature from 1024 to ' + str(reduc_dim))
        # initiate the umap
        umap_init = 'spectral' if(reduc_dim <= (actual_prot_len-2)) else 'random'
        dim_reducer = umap.UMAP(n_components=reduc_dim, low_memory=False, n_epochs=400, densmap=False
                                , init=umap_init, n_neighbors=30, random_state=456, verbose=True)
        # apply fit and transform on the prot_2d_feat_arr
        prot_2d_feat_arr_reduced = dim_reducer.fit_transform(prot_2d_feat_arr)
        # now fix the the protein length at 2000
        print('Now fixing the the protein length at '+ str(fix_length) + '...')
        if actual_prot_len > fix_length:
            prot_2d_feat_arr_reduced =  prot_2d_feat_arr_reduced[:fix_length]
        elif actual_prot_len < fix_length:
            prot_2d_feat_arr_reduced =  np.concatenate((prot_2d_feat_arr_reduced, np.zeros((fix_length - actual_prot_len, reduc_dim))))
        # add the regularized 2d features array to human_seq_feat_2d_reg_dict
        human_seq_feat_2d_reg_dict[prot_id] = prot_2d_feat_arr_reduced
        print('######### completed ' + str(itr) + '-th iteration out of ' + str(total_no_of_proteins-1))
        # after every 3k iterations save the intermediate human_seq_feat_2d_reg_dict, so that
        # in case of the 'for loop' restart, the iterations which are already done can be skipped.  
        if((itr % 3000 == 0) or (itr == total_no_of_proteins - 1)): 
            temp_2d_reg_dict_file_nm = os.path.join(root_path, 'dataset/preproc_data', 'temp_2d_reg_dict_' + str(loop_start_index) + '_' + str(itr) + '.pkl')
            joblib.dump(value=human_seq_feat_2d_reg_dict, filename=temp_2d_reg_dict_file_nm, compress=0)
            # reset human_seq_feat_2d_reg_dict
            human_seq_feat_2d_reg_dict = {}
            # update the relevant variables
            loop_start_index = itr + 1
    # end of for loop

    # save the human_seq_feat_2d_reg_dict as pkl file
    # print("\n Saving human_seq_feat_2d_reg_dict to a .pkl file...")
    # filename = os.path.join(root_path, 'dataset/preproc_data', 'human_seq_feat_2d_reg_dict_' + str(reduc_dim) + '.pkl')
    # joblib.dump(value=human_seq_feat_2d_reg_dict, filename=filename, compress=3)
    # print("\n The human_seq_feat_2d_reg_dict is saved as: " + filename)

def create_prot_len_dist_graph(root_path='./'):
    print('Inside create_prot_len_dist_graph() method - start')
    human_seq_df = pd.read_csv(os.path.join(root_path, 'dataset/preproc_data', 'human_seq.csv'))
    prot_len_arr = human_seq_df['prot_len'].to_numpy()

    # create histogram
    print('creating histogram...')
    plt.style.use('seaborn')  # pretty matplotlib plots
    fig = plt.figure()  # figsize is (width,height) in inches
    plt.title('Cumulative histogram of protein lengths')
    plt.ylabel('cumulative frequency')
    plt.xlabel('protein length')
    n, bins, patches = plt.hist(prot_len_arr, bins = [0, 500, 1000, 1500, 2000, 2500, 3000, 4000, 5500, 7000, 9000, 11000], cumulative=True, histtype='step', color='black', lw=1)
    plt.xticks([0, 1000, 1500, 2000, 2500, 4000, 6000, 8000, 11000], rotation=60)
    # print(n)
    # # display the plot
    # plt.show()
    # save the plot as .png file
    fig.savefig(fname=os.path.join(root_path, 'dataset/preproc_data', 'human_seq_hist.png'), bbox_inches='tight')

    # create boxplot
    print('creating boxplot...')
    plt.style.use('seaborn')  # pretty matplotlib plots
    fig = plt.figure()  # figsize is (width,height) in inches
    plt.title('Boxplot of protein lengths')
    # plt.ylabel('cumulative frequency')
    # plt.xlabel('protein length')
    plt.boxplot(prot_len_arr)
    # # display the plot
    # plt.show()
    # save the plot as .png file
    fig.savefig(fname=os.path.join(root_path, 'dataset/preproc_data', 'human_seq_box.png'), bbox_inches='tight')
    print('Inside create_prot_len_dist_graph() method - end')



if __name__ == '__main__':
    # root_path = os.path.join('/home/Shubh_Working_Ubuntu/Workspaces/PPI_Wkspc/PPI_Code/only_seq_prj_v1')
    # root_path = os.path.join('/home/rs/19CS92W02/Shubh_Working_Remote/PPI_Wkspc/PPI_Code/only_seq_prj_v1')
    root_path = os.path.join('/scratch/pralaycs/Shubh_Working_Remote/PPI_Wkspc/PPI_Code/only_seq_prj_v1')

    # ### DO NOT call below function as it may give problem for the sequence parsing of protein id = 7273. The problem was fixed manually
    # by removing it from everywhere (training as well as test set).
    # ##### parse_human_to_fasta(root_path)

    # preproc_util.extract_feat_from_protTrans(["A E T C Z A O", "S K T Z P"], 
    # protTrans_model_path=os.path.join(root_path, '../ProtTrans_Models/'), protTrans_model_name = 'prot_t5_xl_uniref50')
    # add_protTrans_feat_to_human_seq(root_path
    #                                 ,protTrans_model_path=os.path.join(root_path, '../ProtTrans_Models/')
    #                                 , protTrans_model_name = 'prot_t5_xl_uniref50')

    # regularize the 2d features per protein
    # regularize_prot_2d_feat_AD_HOC(root_path, protTrans_model_name = 'prot_t5_xl_uniref50')

    # genearate graphs for the protein length distribution
    create_prot_len_dist_graph(root_path)